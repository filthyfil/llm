# ğŸ§  GPT-from-Scratch

A minimal, readable implementation of a GPT-style language model built from scratch in PyTorch.  
This project is designed for learning, experimentation, and modification â€” not production deployment.

Shout out to Sebastian Raschka for his work in writing "Build a Large Language Model (From Scratch)"

## ğŸš€ Features

- Implements the GPT architecture (embedding â†’ transformer blocks â†’ output head)
- Multi-head self-attention with causal masking
- Configurable number of layers, heads, embedding dimensions
- Modular design (easily extendable to add KV cache, training loop, etc.)
- Tokenization using OpenAI's `tiktoken`
- Text generation from prompt

## ğŸ“ Project Structure

```
gpt_project/
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ metamorphosis.txt  # Cool book
â”‚   â””â”€â”€ text.txt           # Text generation function
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset.py         # GPTDataset (tokenization + chunking)
â”‚   â””â”€â”€ loader.py          # DataLoader 
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ gpt_model.py       # GPTModel class
â”‚   â”œâ”€â”€ transformer_block.py  # TransformerBlock, LayerNorm, FeedForward
â”‚   â”œâ”€â”€ attention.py       # MultiHeadAttention, CausalSelfAttention
â”‚   â””â”€â”€ activations.py     # GELU activation
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ generate.py        # Text generation function
â”œâ”€â”€ README.md              # You are here!
â”œâ”€â”€ config.py              # Model configuration dictionary
â”œâ”€â”€ main.py                # Entry point for text generation
â””â”€â”€ requirements.txt       # Dependencies
```

## ğŸ§ª Quickstart

1. **Install dependencies**  
Just pytorch and tiktoken; to install my specific versions:
```bash
pip install -r requirements.txt
```

2. **Add your prompt** to `text.txt`

3. **Run the model**
```bash
python main.py
```

The model will read `text.txt`, tokenize it, and generate text token-by-token using causal self-attention.

---

## âš™ï¸ Configuration

Edit `config.py` to modify model hyperparameters:
```python
GPT_CONFIG_124M = {
    "vocab_size": 50257,
    "context_length": 1024,
    "dim_embed": 768,
    "n_heads": 12,
    "n_layers": 12,
    "drop_rate": 0.1,
    "qkv_bias": False
}
```

---

## ğŸ“š Future Additions

- [ ] Training loop (AdamW, LR warmup, etc.)
- [x] KV cache for faster inference
- [ ] Weight initialization strategies
- [ ] Model checkpointing and logging
- [ ] CoT

---

## ğŸ§  Why This Exists

Most open-source GPT repos are large and difficult to read.  
This project is meant to be a **minimalist, hackable reference** for understanding how GPT works under the hood.

---
