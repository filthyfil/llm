# ğŸ§  GPT-from-Scratch

A minimal, readable implementation of a GPT-style language model built from scratch in PyTorch.  
This project is designed for learning, experimentation, and modification â€” not production deployment.

Shout out to Sebastian Raschka for his work in writing "Build a Large Language Model (From Scratch)"

## ğŸš€ Features

- Implements the GPT architecture (embedding â†’ transformer blocks â†’ output head)
- Multi-head self-attention with causal masking
- Configurable number of layers, heads, embedding dimensions
- Modular design (easily extendable to add KV cache, training loop, etc.)
- Tokenization using OpenAI's `tiktoken`
- Text generation from prompt

## ğŸ“ Project Structure

```
gpt_project/
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ metamorphosis.txt  # Cool book
â”‚   â””â”€â”€ text.txt           # Text generation function
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset.py         # GPTDataset (tokenization + chunking)
â”‚   â””â”€â”€ loader.py          # DataLoader 
â”œâ”€â”€ gpt2/                  # Saves raw model parameters and settings 
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ gpt_model.py       # GPTModel class
â”‚   â”œâ”€â”€ transformer_block.py  # TransformerBlock, LayerNorm, FeedForward
â”‚   â”œâ”€â”€ attention.py       # MultiHeadAttention, CausalSelfAttention
â”‚   â””â”€â”€ activations.py     # GELU activation
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ generate.py        # Text generation function
â”‚   â”œâ”€â”€ gpt_download.py    # Set up function, grabs weights online
â”‚   â””â”€â”€ load_weights.py    # Loads weights into the model
â”œâ”€â”€ README.md              # You are here!
â”œâ”€â”€ config.py              # Model configuration dictionary
â”œâ”€â”€ main.py                # Entry point for text generation
â”œâ”€â”€ requirements.txt       # Dependencies
â””â”€â”€ setup.py               # Setup script 
```

## ğŸ§ª Quickstart

1. **Install dependencies**  
Run this command (in a virtual environment):
```bash
pip install -r requirements.txt
```

2. **Install the model**
For testing, install the gpt-small (124M) model.
```bash
python setup.py
```

3. **Run the model**
On the gpt2-small (124M) model the output should read: "The meaning of life is the end. This is not the end of us â€” 
you're never alone" on the input "The meaning of life is".
```bash
python main.py
```
4. **Change the prompt**
You can change the prompt in assets/text.txt

The model will read `text.txt`, tokenize it, and generate text token-by-token using causal self-attention.
On the gpt2-small (124M) model it should read: "The meaning of life is the end. This is not the end of us â€” 
you're never alone" on input "The meaning of life is".

---

## âš™ï¸ Configuration

Edit `config.py` to modify model hyperparameters (Only do this if you know what you are doing!):
```python
GPT_CONFIG_124M = {
    "vocab_size": 50257,
    "context_length": 1024,
    "dim_embed": 768,
    "n_heads": 12,
    "n_layers": 12,
    "drop_rate": 0.1,
    "qkv_bias": False
}
```

---

## ğŸ“š Future Additions

- [ ] Training loop (AdamW, LR warmup, etc.)
- [x] KV cache for faster inference
- [ ] Weight initialization strategies
- [ ] Model checkpointing and logging
- [ ] CoT

---

## ğŸ§  Why This Exists

Most open-source GPT repos are large and difficult to read.  
This project is meant to be a **minimalist, hackable reference** for understanding how GPT works under the hood.

---
